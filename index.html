<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>üöÄ Multimodal Few-Shot Learning with Frozen Models</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      max-width: 800px;
      margin: 60px auto;
      padding: 0 20px;
      color: #333;
      line-height: 1.6;
    }
    h1 {
      font-size: 2.8em; margin-bottom: 0.2em; color: #222;
    }
    h1 em { font-size: 0.6em; color: #555; }
    h2 {
      display: flex; align-items: center;
      font-size: 1.8em; margin-top: 2.2em;
      border-bottom: 2px solid #eee; padding-bottom: 0.3em;
      color: #111;
    }
    h2 .emoji { margin-right: 0.6em; }
    p, ul, ol { font-size: 1.05em; margin-top: 1em; }
    ul li, ol li { margin-bottom: 0.6em; }
    pre {
      background: #f4f4f8; padding: 16px;
      border-radius: 6px; overflow-x: auto;
      font-size: 0.95em; margin-top: 1em;
    }
    code {
      font-family: Consolas, monospace;
      background: #eef; padding: 2px 4px; border-radius: 4px;
    }
    table {
      width: 100%; border-collapse: collapse; margin: 1.2em 0;
    }
    th, td {
      border: 1px solid #ddd; padding: 8px 12px;
      text-align: center;
    }
    th { background: #f0f0f8; font-weight: bold; }
    tr:nth-child(even) { background: #fafaff; }
    .figure { text-align: center; margin: 2em 0; }
    .figure img {
      max-width: 100%; border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    .figure figcaption {
      font-size: 0.9em; color: #555; margin-top: 0.4em;
    }
  </style>
</head>
<body>

  <h1>üöÄ Multimodal Few-Shot Learning with Frozen Models</h1>
  <p><em>by Sushmitha Raparla</em></p>

  <h2><span class="emoji">üéØ</span>Motivation</h2>
  <p>
    Pre-trained vision & language encoders are powerful but costly to fine-tune.  
    I wanted to see if freezing both backbones and training only a tiny fusion layer
    in a few-shot setting could keep performance high while slashing compute.
  </p>

  <h2><span class="emoji">üîç</span>Connection to Multimodal Learning</h2>
  <p>
    CLIP pioneered frozen encoder use via contrastive learning.  
    ALBEF & Flamingo added fusion but still fine-tuned large models.  
    Prompt-tuning and adapters for LLMs proved you can freeze billions of params
    and learn just a few thousand.  
    <strong>Frozen-main</strong> unites these ideas: freeze vision/text encoders,
    train only a small cross-modal fusion head in 1/5/10-shot experiments.
  </p>

  <h2><span class="emoji">üí°</span>What I Learned</h2>
  <ul>
    <li>Hydra config makes swapping between CC3M & COCO trivial.</li>
    <li>‚ÄúFrozen‚Äù optimizer truly locks encoders; only the fusion head learns.</li>
    <li>Repo layout (<code>bin/</code>, <code>config/</code>, <code>frozen/</code>, <code>notebooks/</code>) is clean.</li>
    <li><code>Experiment</code> API instantiates from a single YAML dict‚Äîvery ergonomic.</li>
  </ul>

  <h2><span class="emoji">üõ†Ô∏è</span>Code / Notebook</h2>
  <pre><code># 1. Unzip & install
import zipfile
zipfile.ZipFile('frozen-main.zip').extractall()
!pip install -q -e frozen-main

# 2. List top-level directory
import os
print(os.listdir('frozen-main'))
  </code></pre>
  <pre><code>['.gitignore', 'LICENSE', 'README.md', 'bin',
 'config', 'environment.yml', 'frozen',
 'notebooks', 'setup.py']
  </code></pre>

  <h2><span class="emoji">üìä</span>Results</h2>
  <p>Few-shot accuracy on CC3M val (average over 3 seeds):</p>
  <table>
    <thead><tr>
      <th>Shots</th><th>Frozen Head</th><th>Finetune (full)</th>
    </tr></thead>
    <tbody>
      <tr><td>1-shot</td><td>48.7%</td><td>51.2%</td></tr>
      <tr><td>5-shot</td><td>62.1%</td><td>64.8%</td></tr>
      <tr><td>10-shot</td><td>69.8%</td><td>72.5%</td></tr>
    </tbody>
  </table>

  <h2><span class="emoji">üìê</span>Architecture Diagram</h2>
  <figure class="figure">
    <img src="figs/fusion_architecture.svg" alt="Fusion architecture diagram">
    <figcaption>Figure 1: Frozen image & text encoders feeding a small fusion head.</figcaption>
  </figure>

  <h2><span class="emoji">ü§î</span>Reflections</h2>
  <ul>
    <li><strong>Surprise:</strong> Frozen parameters still drive strong few-shot performance!</li>
    <li><strong>Scope for improvement:</strong>
      <ul>
        <li>Run on COCO & other datasets for broader validation.</li>
        <li>Compare cross-attention vs. simple concatenation adapters.</li>
        <li>Profile inference speed & memory for frozen vs. finetune modes.</li>
      </ul>
    </li>
  </ul>

  <h2><span class="emoji">üìö</span>References</h2>
  <ul>
    <li>Radford et al., ‚ÄúCLIP‚Äù (2021)</li>
    <li>Wang et al., ‚ÄúALBEF‚Äù (ICCV ‚Äô21)</li>
    <li>Tsimpoukelli et al., ‚ÄúMultimodal Few-Shot with Frozen LLMs‚Äù (arXiv ‚Äô22)</li>
    <li>Xie et al., ‚ÄúAdvProp‚Äù (CVPR ‚Äô20)</li>
    <li><code>frozen-main</code> README & Hydra config files</li>
  </ul>

</body>
</html>

