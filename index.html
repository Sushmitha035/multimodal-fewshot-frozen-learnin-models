<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Multimodal Few-Shot Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      display: flex;
    }

    /* Container for sidebar and main content */
    .container {
      display: flex;
      width: 100%;
      margin: 0;
      padding: 20px;
    }

    /* Sidebar Styling */
    .sidebar {
      width: 25%;
      background-color: #fff;
      padding: 20px;
      box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);
    }

    .sidebar h1 {
      font-size: 2em;
      color: #4A90E2;
      margin-top: 0;
    }

    .sidebar a {
      display: block;
      color: #4A90E2;
      text-decoration: none;
      font-size: 1.1em;
      margin: 10px 0;
    }

    .sidebar a:hover {
      text-decoration: underline;
    }

    .sidebar hr {
      border: 1px solid #ddd;
      margin: 20px 0;
    }

    /* Main content styling */
    .main {
      width: 75%;
      padding: 20px;
    }

    header {
      text-align: center;
      padding: 50px 0;
      background-color: #fff;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }

    h1 {
      font-size: 3em;
      color: #333;
      margin-bottom: 10px;
    }

    h1 small {
      font-size: 0.8em;
      color: #777;
      font-weight: 400;
    }

    h2 {
      font-size: 2.5em;
      margin-top: 40px;
      color: #444;
    }

    p, li {
      font-size: 1.1em;
      margin-bottom: 20px;
      color: #555;
    }

    ul {
      margin-left: 20px;
    }

    pre {
      background-color: #f4f4f4;
      padding: 15px;
      border-radius: 8px;
      overflow-x: auto;
    }

    code {
      font-family: Consolas, monospace;
      background-color: #f4f4f4;
      padding: 5px 10px;
      border-radius: 5px;
    }

    img {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      margin-top: 20px;
    }

    .card {
      background-color: #fff;
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
    }

    footer {
      text-align: center;
      padding: 20px 0;
      background-color: #f9f9f9;
      margin-top: 40px;
      color: #777;
    }

    .section-title {
      font-size: 1.8em;
      border-bottom: 2px solid #eee;
      padding-bottom: 10px;
      margin-top: 40px;
    }

    .content-between {
      display: flex;
      justify-content: space-between;
      margin-top: 20px;
      gap: 20px;
    }

    .content-between img {
      max-width: 48%;
    }
  </style>
</head>
<body>

  <div class="container">
    <!-- Sidebar -->
    <div class="sidebar">
      <h1>Sushmitha035</h1>
      <p><a href="https://github.com/Sushmitha035" target="_blank">View My GitHub Profile</a></p>
      <hr />
      <a href="#overview">Overview</a>
      <a href="#few-shot">Few-Shot Learning</a>
      <a href="#motivation">Motivation</a>
      <a href="#fusion">Fusion</a>
      <a href="#results">Results</a>
      <a href="#code">Code</a>
      <a href="#conclusion">Conclusion</a>
      <a href="#references">References</a>
      <hr />
     <a href="https://huggingface.co/datasets/laion/cc3m" target="_blank">ğŸ“‚ CC3M Dataset</a><br/>
  <a href="https://cocodataset.org/" target="_blank">ğŸ“‚ COCO Dataset</a>

    </div>

    <!-- Main Content -->
    <div class="main">
      <header>
        <h1>Multimodal Few-Shot Learning with Frozen Language Models ğŸ§  <small>By Raparla Sushmitha</small></h1>
        <p><em><strong>Date: 08-05-2027</strong></em></p>
      </header>

      <h2 class="section-title" id="overview">Overview ğŸ“š</h2>
      <p>This project explores multimodal few-shot learning using frozen encoders. Traditional ML models rely on extensive datasets, whereas this work shows how models like CLIP can generalize with minimal data when properly fused.</p>

      <div class="card">
        <h3>Highlights ğŸ’¡</h3>
        <ul>
          <li>Efficient learning from limited examples</li>
          <li>Frozen CLIP encoders eliminate expensive fine-tuning</li>
          <li>Low compute requirement</li>
          <li>Adaptable to unseen tasks</li>
        </ul>
      </div>
 </p>
<h2 class="section-title">Motivation: Why This Topic? ğŸ’­</h2>
    <p>
      Pre-trained models like CLIP have demonstrated outstanding performance on various vision-language tasks without needing extensive fine-tuning. However, these models come with high computational costs, making them difficult to deploy in resource-constrained environments. I was intrigued by the possibility of using these models without the need for expensive fine-tuning and still achieving competitive results. ğŸ¤”
    </p>
    <p>
      The primary motivation behind this project is to explore how frozen models, such as CLIP, can be used in a few-shot learning setting to perform vision-language tasks. By reducing the computational cost and making models more accessible, this project aims to push the boundaries of multimodal learning in real-world applications, where data and resources are often limited. ğŸ’ª
    </p>
  
<p>
      <h2 class="section-title" id="few-shot">What is Few-Shot Learning ğŸ”</h2>
      <p>Few-shot learning refers to the ability to generalize from just a handful of examples. This is crucial in domains like medical imaging or low-resource languages.</p>
<p>
      The primary motivation behind this project is to explore how frozen models, such as CLIP, can be used in a few-shot learning setting to perform vision-language tasks. By reducing the computational cost and making models more accessible, this project aims to push the boundaries of multimodal learning in real-world applications, where data and resources are often limited. ğŸ’ª
    </p>
          </p>
<h2 class="section-title"Combining Few-Shot Learning and Frozen Language Models ğŸ”</h2>
    <p>
       In this project, we combine the power of few-shot learning with frozen pre-trained language models. By using models like CLIP that are already pre-trained on vast amounts of data, we leverage their ability to handle both visual and textual data without the need for expensive retraining.
    </p>
  </p>
    <h2 class="section-title">Fusion Strategies ğŸ”—</h2>
    <p>
      Fusion techniques refer to combining visual and textual features. This can be early, late, or hybrid. We opted for a lightweight late fusion approach to minimize resource use.
    </p>

    <h2 class="section-title">How It Connects with Past and Current Work ğŸ”„</h2>
    <p>
      Our work builds on OpenAIâ€™s CLIP (Contrastive Languageâ€“Image Pretraining), which maps images and texts to a shared embedding space. Other recent multimodal approaches include Flamingo, BLIP, and ALBEF.
    </p>
    <p>
      While Flamingo uses a causal decoder and visual tokens, our approach sticks to frozen models and explores simpler fusion, saving compute. Compared to ALBEF, we avoid heavy vision-language pre-training.
    </p>
  </p>
  <h2 class="section-title">ğŸ“˜ What I Learned From This Project</h2>
<ul>
  <li><strong>Understanding Frozen Models:</strong> I deepened my knowledge of how frozen models like CLIP function and their advantages in low-resource settings.</li>
  <li><strong>Efficiency in Learning:</strong> Learned how to use few-shot learning effectively without expensive training or fine-tuning, enabling faster experimentation.</li>
  <li><strong>Fusion Matters:</strong> Realized that even simple fusion strategies can lead to competitive performance when thoughtfully designed.</li>
  <li><strong>Code Modularity:</strong> Building a modular and reusable codebase made it easier to tweak experiments and analyze results quickly.</li>
  <li><strong>Balancing Performance vs. Compute:</strong> Understood the trade-offs between model performance and computational cost, especially important for real-world deployments.</li>
  <li><strong>Importance of Evaluation:</strong> Gained experience in evaluating models across different shot settings and interpreting accuracy trends.</li>
</ul>

    <h2 class="section-title">Key Learnings and Insights ğŸ“</h2>
    <ul>
      <li>Frozen models are still highly effective for multimodal tasks, and they offer a more efficient alternative to fully fine-tuned models. ğŸŒ±
      </li>
      <li>Simple fusion methods can be highly effective for cross-modal learning, which challenges the need for complex architectures in certain tasks. ğŸ”</li>
      <li>Few-shot learning can be applied with minimal updates to the model, making it highly efficient for tasks with limited labeled data. ğŸ“¦
      </li>
      <li>The modular design of the codebase allows for flexible experimentation, which is key to advancing multimodal learning in a more accessible manner. ğŸ”„</li>
    </ul>
    <h2 class="section-title">Code Setup & Installation ğŸ’»</h2>
    <pre><code>!unzip frozen-main.zip
!pip install -q -e frozen-main</code></pre>

    <pre><code>import os
print("Contents of frozen-main directory:")
print(os.listdir('frozen-main'))</code></pre>

    <h2 class="section-title">Training Loop Snippet ğŸ› ï¸</h2>
    <pre><code>from frozen_main import trainer

for shot in [1, 5, 10]:
    print(f"Running {shot}-shot training...")
    acc = trainer.run_few_shot(shot=shot)
    print(f"Accuracy for {shot}-shot: {acc}%")</code></pre>
  

    <h2 class="section-title">Few-Shot Evaluation Datasets ğŸ“‚</h2>
<p>
  We evaluated our few-shot learning setup on two widely-used vision-language datasets: <strong>CC3M</strong> and <strong>COCO</strong>. These provide a solid benchmark for comparing performance under limited data conditions.
</p>

<h2 class="section-title">Few-Shot Results on CC3M ğŸ“Š</h2>
<p>Below are few-shot results measured on the CC3M dataset:</p>


<h2>Model Performance Comparison</h2>

<table style="border-collapse: collapse; width: 50%;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">Shots</th>
      <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">Frozen Head</th>
      <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">Full Finetune</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">1-shot</td>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">48.7%</td>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">51.2%</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">5-shot</td>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">62.1%</td>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">64.8%</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">10-shot</td>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">69.8%</td>
      <td style="border: 1px solid black; padding: 8px; text-align: center;">72.5%</td>
    </tr>
  </tbody>
</table>
      <div class="content-between">
        <img src="few_shot_performance.png" alt="Performance Chart">
        <img src="few_shot_accuracy_chart.png" alt="Accuracy Chart">
      </div>

      <h2 class="section-title" id="code">Code Setup & Installation ğŸ’»</h2>
      <pre><code>!unzip frozen-main.zip
!pip install -q -e frozen-main</code></pre>
 <h2 class="section-title">What Surprised Me ğŸ˜²</h2>
    <ul>
      <li>Frozen models outperformed expectations, especially in the 5-shot case.</li>
      <li>Simple cosine similarity worked better than MLPs in some cases.</li>
      <li>Zero-shot performance was already competitive with few-shot SOTA.</li>
    </ul>
<h2 class="section-title">Scope for Improvement ğŸš€</h2>
    <ul>
      <li>Incorporating better fusion methods like gated attention mechanisms</li>
      <li>Testing on more challenging datasets such as VQA or NLVR2</li>
      <li>Introducing prompt tuning for better generalization</li>
      <li>Combining audio and video for true multimodality</li>
    </ul>
  <h2 class="section-title">Full Evaluation Script ğŸ§ª</h2>
    <pre><code>from frozen_main.eval import evaluate_all

def run_all_experiments():
    for shot in [1, 5, 10]:
        print(f"Evaluating {shot}-shot performance...")
        results = evaluate_all(shot=shot)
        print(results)</code></pre>
<h2 class="section-title">Bonus: Model Architecture Overview ğŸ—ï¸</h2>
    <pre><code>Vision Encoder (CLIP)
    |
    --> Linear Projection --> Feature Embedding
Text Encoder (CLIP)
    |
    --> Linear Projection --> Feature Embedding
    |
    --> Cosine Similarity --> Prediction</code></pre>

    <h2 class="section-title">Future Work ğŸ”®</h2>
    <ul>
      <li>Explore parameter-efficient tuning methods like LoRA</li>
      <li>Investigate transfer learning across domains</li>
      <li>Apply to multilingual settings for global accessibility</li>
    </ul>

      <h2 class="section-title" id="conclusion">Conclusion & Benefits ğŸ</h2>
      <p>Frozen models offer an efficient path for multimodal learning. Our findings show that high accuracy is achievable with minimal tuning.</p>
      <p>This research bridges the gap between state-of-the-art models and real-world applicability in low-resource scenarios. It opens the door to accessible and efficient AI.</p>

      <h2 class="section-title" id="references">ğŸ“š References</h2>
      <ul>
        <li><a href="https://arxiv.org/abs/2106.13884" target="_blank">CLIP Paper - Radford et al. (2021)</a></li>
        <li>Wang et al., â€œALBEFâ€ (ICCV â€™21)</li>
        <li>Tsimpoukelli et al., â€œFrozen: Learning with Frozen LMsâ€ (2022)</li>
        <li>Xie et al., â€œAdvPropâ€ (CVPR â€™20)</li>
        <li>Huang et al., â€œFlamingoâ€ (DeepMind, 2022)</li>
      </ul>

    </div>
  </div>
</body>
</html>

