<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Multimodal Few-Shot Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background: #fdfdfd;
      color: #222;
      line-height: 1.7;
      max-width: 900px;
      margin: 0 auto;
      padding: 40px 20px;
    }
    h1 {
      font-size: 2.5em;
      margin-bottom: 0.2em;
    }
    h1 small {
      font-weight: normal;
      display: block;
      font-size: 0.6em;
      color: #666;
    }
    h2 {
      margin-top: 2.5em;
      color: #333;
    }
    p, ul {
      font-size: 1.1em;
    }
    pre {
      background: #f4f4f4;
      padding: 12px;
      border-radius: 6px;
      overflow-x: auto;
    }
    code {
      font-family: Consolas, monospace;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 20px 0;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 10px;
      text-align: center;
    }
    th {
      background: #f0f0f0;
    }
    img {
      max-width: 100%;
      margin: 20px 0;
      border: 1px solid #ddd;
      border-radius: 6px;
    }
    em {
      color: #555;
    }
    li {
      margin-bottom: 8px;
    }
  </style>
</head>
<body>

  <h1>Multimodal Few-Shot Learning<br><small>Frozen Encoders, Fresh Ideas</small></h1>
  <p><em>July 2025 Â· Raparla Sushmitha</em></p>

  <h2>ğŸ¯ Motivation</h2>
  <p>
    Pre-trained vision and language encoders like CLIP are incredibly powerfulâ€”but fine-tuning them can be expensive.
    We ask: <strong>can we achieve strong few-shot performance by freezing these models and training only a lightweight fusion head?</strong>
  </p>
  <p>Spoiler: yes, we can.</p>

  <h2>ğŸ” Rethinking Multimodal Learning</h2>
  <p>
    CLIPâ€™s zero-shot ability was a turning point for multimodal models. Later methods like ALBEF and Flamingo introduced fusion mechanisms but still required fine-tuning large backbones. 
    Inspired by prompt-tuning and adapter methods, we explore a minimalist approach: <strong>frozen vision and text encoders, train only a cross-modal fusion head</strong>.
  </p>
  <p>This setup drastically reduces computational costâ€”while still delivering competitive results.</p>

  <h2>ğŸ“š Historical Context</h2>
  <p>
    Multimodal learning has evolved from late fusion pipelines to deeply integrated models. CLIP by OpenAI (2021) unified vision and text via contrastive pretraining, enabling zero-shot capabilities. 
    ALBEF (ICCV 2021) pushed this further by using momentum distillation. More recently, methods like Flamingo (DeepMind, 2022) blend frozen LLMs with learned visual adapters. Our method extends this trajectory:
    <strong>minimal updates, maximum reuse.</strong>
  </p>

  <h2>ğŸ§  What We Learned</h2>
  <ul>
    <li><strong>Frozen backbones</strong> are extremely effective in few-shot settings.</li>
    <li><strong>Cross-modal fusion</strong> doesnâ€™t need to be deepâ€”2â€“3 layers are often enough.</li>
    <li><strong>Reproducibility</strong> improves when fewer parameters are updated.</li>
    <li>CLIPâ€™s representations are rich enough to generalize with minimal tuning.</li>
  </ul>

  <h2>ğŸ’» Code & Setup</h2>
  <pre><code># Install the code
!unzip frozen-main.zip
!pip install -q -e frozen-main</code></pre>

  <pre><code># Project layout
import os
print(os.listdir('frozen-main'))
# ['.gitignore', 'LICENSE', 'README.md', 'bin',
#  'config', 'environment.yml', 'frozen',
#  'notebooks', 'setup.py']</code></pre>

  <p>
    The training is modular thanks to <strong>Hydra</strong> configuration files. 
    Here's how we train using CC3M data with frozen encoders and a 3-layer fusion head.
  </p>

  <pre><code># Example config call
python bin/train --config-name=train dataset=CC3M optimizer=Finetune</code></pre>

  <p>Each run logs accuracy per shot setting and tracks validation loss. We use OPT-style language models for the text branch and CLIP ViT-B/32 for vision.</p>

  <h2>ğŸ“Š Results</h2>
  <p>
    We evaluate on the CC3M dataset using three random seeds. Even with just a frozen encoder and a tiny fusion head, results are surprisingly strong:
  </p>

  <table>
    <thead>
      <tr>
        <th>Shots</th>
        <th>Frozen Head</th>
        <th>Full Finetune</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>1-shot</td><td>48.7%</td><td>51.2%</td></tr>
      <tr><td>5-shot</td><td>62.1%</td><td>64.8%</td></tr>
      <tr><td>10-shot</td><td>69.8%</td><td>72.5%</td></tr>
    </tbody>
  </table>

  <img src="few_shot_accuracy_chart.png" alt="Few-Shot Accuracy Chart" />

  <h2>ğŸ¤  Reflections</h2>
  <p><strong>What surprised us:</strong></p>
  <ul>
    <li>High performance with <strong>frozen encoders</strong>â€”we expected more degradation.</li>
    <li>Fusion heads generalized well across datasets with minimal tuning.</li>
    <li>Inference time dropped significantly due to smaller trainable modules.</li>
  </ul>
  <p><strong>Scope for improvement:</strong></p>
  <ul>
    <li>Explore other fusion designs like FiLM or LoRA-infused adapters.</li>
    <li>Incorporate larger LLMs to improve text comprehension in noisy tasks.</li>
    <li>Benchmark on real-world settings like VQA, Captioning, and Retrieval.</li>
  </ul>

  <h2>ğŸ“ Next Steps</h2>
  <ul>
    <li>â†» Try on COCO, VQA, and ImageNet.</li>
    <li>ğŸ” Compare fusion strategies: concatenation vs. cross-attention.</li>
    <li>ğŸš€ Benchmark latency and memory for real-world applications.</li>
  </ul>

  <h2>ğŸ“š References</h2>
  <ul>
    <li><a href="https://arxiv.org/abs/2106.13884" target="_blank">Tsimpoukelli et al., â€œMultimodal Few-Shot Learning with Frozen LMsâ€, 2021</a></li>
    <li><a href="https://arxiv.org/abs/2205.01068" target="_blank">Zhang et al., â€œOPT: Open Pretrained Transformersâ€, 2022</a></li>
    <li><a href="https://arxiv.org/abs/2103.00020" target="_blank">Radford et al., â€œCLIPâ€, 2021</a></li>
    <li><a href="https://arxiv.org/abs/2107.07651" target="_blank">Wang et al., â€œALBEFâ€, ICCV 2021</a></li>
    <li><a href="https://arxiv.org/abs/2302.10866" target="_blank">DeepMind Flamingo, 2022</a></li>
  </ul>

</body>
</html>


 


