<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Multimodal Few-Shot Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background: #fdfdfd;
      color: #222;
      line-height: 1.7;
      max-width: 900px;
      margin: 0 auto;
      padding: 40px 20px;
    }
    h1 {
      font-size: 2.5em;
      margin-bottom: 0.2em;
    }
    h1 small {
      font-weight: normal;
      display: block;
      font-size: 0.6em;
      color: #666;
    }
    h2 {
      margin-top: 2.5em;
      color: #333;
    }
    p, ul {
      font-size: 1.1em;
    }
    pre {
      background: #f4f4f4;
      padding: 12px;
      border-radius: 6px;
      overflow-x: auto;
    }
    code {
      font-family: Consolas, monospace;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 20px 0;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 10px;
      text-align: center;
    }
    th {
      background: #f0f0f0;
    }
    img {
      max-width: 100%;
      margin: 20px 0;
      border: 1px solid #ddd;
      border-radius: 6px;
    }
    em {
      color: #555;
    }
    li {
      margin-bottom: 8px;
    }
    .card {
      border: 1px solid #ccc;
      border-radius: 8px;
      padding: 16px;
      margin: 16px 0;
      background: #fff;
    }
    .card h3 {
      margin-top: 0;
    }
    .section-title {
      margin-top: 3em;
      border-bottom: 2px solid #eee;
      padding-bottom: 8px;
    }
  </style>
</head>
<body>

  <!-- PART 1: Your Original Page Layout -->

  <h1>Multimodal Few-Shot Learning <small>Frozen Encoders, Fresh Ideas</small></h1>
  <p><em>By Raparla Sushmitha</em></p>

  <h2 class="section-title">About the Project</h2>
  <p>
    This project explores how frozen pre-trained vision and language encoders can be adapted for few-shot multimodal learning tasks. We investigate lightweight fusion modules, performance across different shots, and compare frozen vs. full fine-tuning methods.
  </p>

  <div class="card">
    <h3>Highlights</h3>
    <ul>
      <li>Uses CLIP & Frozen LMs for vision-language alignment</li>
      <li>Few-shot learning on CC3M dataset</li>
      <li>Modular codebase for rapid experimentation</li>
    </ul>
  </div>

  <div class="card">
    <h3>Technologies Used</h3>
    <ul>
      <li>PyTorch, HuggingFace Transformers</li>
      <li>CLIP, Flamingo-style Fusion, Adapter layers</li>
      <li>WandB for experiment tracking</li>
    </ul>
  </div>

  <!-- PART 2: Blog-Style Post (from GitHub Screenshot) -->

  <h2 class="section-title">üìå Title & Your Name</h2>
  <p><strong>Multimodal Few-Shot Learning: Frozen Encoders, Fresh Ideas</strong><br>Raparla Sushmitha</p>

  <h2>üéØ Motivation ‚Äì Why did you pick this topic?</h2>
  <p>
    Pre-trained vision and language encoders like CLIP are incredibly powerful‚Äîbut fine-tuning them can be expensive.
    I was fascinated by the idea of leveraging these models as-is and still achieving good performance. The challenge of improving few-shot learning without costly training drove me to this topic.
  </p>

  <h2>üîÅ How does it connect with past and current work?</h2>
  <p>
    This work draws from CLIP‚Äôs success in zero-shot vision-language tasks and expands it with ideas from Flamingo and ALBEF.
    Where those models require full fine-tuning, we take inspiration from prompt tuning and adapters to push for efficient learning using frozen backbones.
    This fits into the broader trend in multimodal learning that seeks to reduce compute while improving generalization.
  </p>

  <h2>üìò Explain your learning from this work</h2>
  <ul>
    <li>Frozen models retain rich representations that are highly reusable.</li>
    <li>Cross-modal fusion can be simple but still effective.</li>
    <li>Few-shot learning doesn't need large updates‚Äîjust smart ones.</li>
    <li>Modular experiment configs make testing faster and reproducible.</li>
  </ul>

  <h2>üíª Code / Notebook</h2>
  <p>Install the codebase and experiment:</p>
  <pre><code>!unzip frozen-main.zip
!pip install -q -e frozen-main</code></pre>
  <pre><code>import os
print(os.listdir('frozen-main'))</code></pre>

  <h2>üìä Results</h2>
  <p>
    Evaluated on CC3M dataset under few-shot settings:
  </p>

  <table>
    <thead>
      <tr>
        <th>Shots</th>
        <th>Frozen Head</th>
        <th>Full Finetune</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>1-shot</td><td>48.7%</td><td>51.2%</td></tr>
      <tr><td>5-shot</td><td>62.1%</td><td>64.8%</td></tr>
      <tr><td>10-shot</td><td>69.8%</td><td>72.5%</td></tr>
    </tbody>
  </table>

  <img src="few_shot_accuracy_chart.png" alt="Few-Shot Accuracy Chart" />

  <h2>üß† Reflections</h2>
  <p><strong>(a) What surprised you?</strong></p>
  <ul>
    <li>Frozen encoders still performed extremely well‚Äîcloser than expected to full fine-tuning.</li>
    <li>Training only the fusion module drastically reduced training time.</li>
  </ul>

  <p><strong>(b) What can be improved?</strong></p>
  <ul>
    <li>More advanced fusion techniques may further improve accuracy.</li>
    <li>Try with newer datasets or LLM-based vision-language encoders.</li>
    <li>Better visualizations to understand cross-modal alignment.</li>
  </ul>

  <h2>üìö References</h2>
  <ul>
    <li>Radford et al., ‚ÄúCLIP‚Äù (2021) ‚Äì <a href="https://openai.com/research/clip">link</a></li>
    <li>Wang et al., ‚ÄúALBEF‚Äù (ICCV ‚Äô21)</li>
    <li>Tsimpoukelli et al., ‚ÄúFrozen: Learning with Frozen LMs‚Äù (2022)</li>
    <li>Xie et al., ‚ÄúAdvProp‚Äù (CVPR ‚Äô20)</li>
    <li>Huang et al., ‚ÄúFlamingo‚Äù (DeepMind, 2022)</li>
  </ul>

</body>
</html>


 


