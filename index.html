<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Multimodal Few-Shot Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      margin: 0;
      padding: 0;
      line-height: 1.6;
    }
    .container {
      width: 100%;
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
    }
    header {
      text-align: center;
      padding: 50px 0;
      background-color: #fff;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    h1 {
      font-size: 3em;
      color: #333;
      margin-bottom: 10px;
    }
    h1 small {
      font-size: 0.8em;
      color: #777;
      font-weight: 400;
    }
    h2 {
      font-size: 2.5em;
      margin-top: 40px;
      color: #444;
    }
    p {
      font-size: 1.1em;
      margin-bottom: 20px;
      color: #555;
    }
    ul {
      font-size: 1.1em;
      margin: 20px 0;
    }
    ul li {
      margin-bottom: 10px;
    }
    pre {
      background-color: #f4f4f4;
      padding: 15px;
      border-radius: 8px;
      overflow-x: auto;
    }
    code {
      font-family: Consolas, monospace;
      background-color: #f4f4f4;
      padding: 5px 10px;
      border-radius: 5px;
    }
    img {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      margin-top: 20px;
    }
    .card {
      background-color: #fff;
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
    }
    .card h3 {
      margin-top: 0;
      color: #333;
    }
    .section-title {
      font-size: 1.8em;
      border-bottom: 2px solid #eee;
      padding-bottom: 10px;
      margin-top: 40px;
    }
    footer {
      text-align: center;
      padding: 20px 0;
      background-color: #f9f9f9;
      margin-top: 40px;
      color: #777;
    }
  </style>
</head>
<body>

  <div class="container">
    <header>
      <h1>Multimodal Few-Shot Learning <small>Leveraging Frozen Encoders for Efficient Vision-Language Tasks</small></h1>
      <p><em>By Raparla Sushmitha</em></p>
    </header>

    <h2 class="section-title">About the Project</h2>
    <p>
      This project delves into the exciting world of multimodal few-shot learning, where we aim to use pre-trained models to solve vision-language tasks with minimal data. Traditional machine learning models require large amounts of labeled data, but few-shot learning seeks to break this barrier, enabling effective performance with just a few examples. Our work focuses on adapting frozen models (such as CLIP) for these tasks, ensuring that they can still perform at a high level even without being fine-tuned on every new dataset.
    </p>
    <p>
      Specifically, we explore the use of frozen pre-trained vision and language encoders, like CLIP, in combination with lightweight fusion strategies. These strategies enable the model to perform multimodal tasks efficiently, reducing both the computational cost and the need for large-scale retraining. The goal is to create an efficient and scalable solution for vision-language alignment in few-shot settings.
    </p>

    <div class="card">
      <h3>Key Highlights</h3>
      <ul>
        <li>Using CLIP and other frozen language models for vision-language alignment tasks.</li>
        <li>Few-shot learning experiments conducted on the large-scale CC3M dataset.</li>
        <li>Evaluation of the impact of frozen vs. fully fine-tuned models in various few-shot settings.</li>
        <li>Exploring the efficiency of modular and lightweight fusion strategies for multimodal learning.</li>
      </ul>
    </div>

    <div class="card">
      <h3>Technologies and Tools Used</h3>
      <ul>
        <li>PyTorch for building and experimenting with machine learning models.</li>
        <li>HuggingFace Transformers for leveraging pre-trained language models like CLIP.</li>
        <li>WandB (Weights and Biases) for experiment tracking and hyperparameter optimization.</li>
        <li>CLIP for vision-language pretraining, which allows models to align textual and visual data.</li>
        <li>Flamingo-style fusion and adapter layers for modular learning without full fine-tuning.</li>
      </ul>
    </div>

    <h2 class="section-title">Motivation: Why This Topic?</h2>
    <p>
      Pre-trained models like CLIP have demonstrated outstanding performance on various vision-language tasks without needing extensive fine-tuning. However, these models come with high computational costs, making them difficult to deploy in resource-constrained environments. I was intrigued by the possibility of using these models without the need for expensive fine-tuning and still achieving competitive results.
    </p>
    <p>
      The primary motivation behind this project is to explore how frozen models, such as CLIP, can be used in a few-shot learning setting to perform vision-language tasks. By reducing the computational cost and making models more accessible, this project aims to push the boundaries of multimodal learning in real-world applications, where data and resources are often limited.
    </p>

    <h2 class="section-title">Connection to Past and Current Work</h2>
    <p>
      This work builds on the success of CLIP, which has already proven its ability to handle zero-shot vision-language tasks. In comparison to models that require extensive fine-tuning, such as Flamingo or ALBEF, our work leverages prompt tuning and adapter-based methods to reduce computational requirements while retaining high performance.
    </p>
    <p>
      Our approach is aligned with the growing trend in machine learning to reduce the compute required for fine-tuning models, which has significant implications for scalability and efficiency in deploying AI systems.
    </p>

    <h2 class="section-title">Key Learnings and Insights</h2>
    <ul>
      <li>Frozen models are still highly effective for multimodal tasks, and they offer a more efficient alternative to fully fine-tuned models.</li>
      <li>Simple fusion methods can be highly effective for cross-modal learning, which challenges the need for complex architectures in certain tasks.</li>
      <li>Few-shot learning can be applied with minimal updates to the model, making it highly efficient for tasks with limited labeled data.</li>
      <li>The modular design of the codebase allows for flexible experimentation, which is key to advancing multimodal learning in a more accessible manner.</li>
    </ul>

    <h2 class="section-title">Code and Experimentation</h2>
    <p>Install the codebase and start experimenting with the models:</p>
    <pre><code>!unzip frozen-main.zip
!pip install -q -e frozen-main</code></pre>
    <pre><code>import os
print(os.listdir('frozen-main'))</code></pre>

    <h2 class="section-title">Results and Performance</h2>
    <p>
      The results show that frozen models can perform nearly as well as fully fine-tuned models in few-shot settings, especially when evaluated on the CC3M dataset. The performance across different shot settings (1-shot, 5-shot, and 10-shot) reveals that frozen models can achieve impressive results with minimal data.
    </p>

    <table>
      <thead>
        <tr>
          <th>Shots</th>
          <th>Frozen Head</th>
          <th>Full Finetune</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>1-shot</td><td>48.7%</td><td>51.2%</td></tr>
        <tr><td>5-shot</td><td>62.1%</td><td>64.8%</td></tr>
        <tr><td>10-shot</td><td>69.8%</td><td>72.5%</td></tr>
      </tbody>
    </table>

    <img src="few_shot_performance.png" alt="Few-Shot Learning Performance Chart" />
    <img src="few_shot_accuracy_chart.png" alt="Few-Shot Accuracy Chart" />

    <h2 class="section-title">Reflections and Future Work</h2>
    <p><strong>What surprised you?</strong></p>
    <ul>
      <li>The performance of frozen encoders was much better than expected, with results very close to those of full fine-tuning.</li>
      <li>Using a fusion module instead of fine-tuning the entire model drastically reduced the training time.</li>
    </ul>

    <p><strong>What can be improved?</strong></p>
     <ul>
    <li>More advanced fusion techniques may further improve accuracy.</li>
    <li>Try with newer datasets or LLM-based vision-language encoders.</li>
    <li>Better visualizations to understand cross-modal alignment.</li>
  </ul>

 <h2>üìö References</h2>
<ul>
    <li>For more details on CLIP, refer to the paper by Radford et al. (2021) available on arXiv: 
        <a href="https://arxiv.org/abs/2106.13884" target="_blank">CLIP Paper - Radford et al. (2021)</a>
    </li>
    <li>Wang et al., ‚ÄúALBEF‚Äù (ICCV ‚Äô21)</li>
    <li>Tsimpoukelli et al., ‚ÄúFrozen: Learning with Frozen LMs‚Äù (2022)</li>
    <li>Xie et al., ‚ÄúAdvProp‚Äù (CVPR ‚Äô20)</li>
    <li>Huang et al., ‚ÄúFlamingo‚Äù (DeepMind, 2022)</li>
</ul>

</body>
</html> 
    <ul>
      <li>Implementing more sophisticated fusion techniques could


 


