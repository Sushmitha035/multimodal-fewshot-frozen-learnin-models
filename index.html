<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Multimodal Few-Shot Learning with Frozen Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: 'Georgia', serif;
      background: #fff;
      color: #333;
      line-height: 1.8;
      max-width: 900px;
      margin: 80px auto;
      padding: 0 40px;
      font-size: 18px;
    }

    h1 {
      font-size: 3em;
      font-weight: normal;
      margin-bottom: 0.1em;
    }

    h1 em {
      display: block;
      font-size: 0.4em;
      color: #666;
      margin-top: 0.2em;
    }

    h2 {
      font-size: 1.8em;
      margin-top: 2em;
      border-bottom: 1px solid #ddd;
      padding-bottom: 0.2em;
    }

    ul, ol {
      margin-left: 1.5em;
    }

    pre {
      background: #f8f8f8;
      padding: 16px;
      overflow-x: auto;
      border-left: 4px solid #ccc;
      font-size: 0.95em;
    }

    code {
      font-family: Menlo, Consolas, monospace;
      background: #f0f0f0;
      padding: 2px 5px;
      border-radius: 3px;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1em;
      font-size: 0.95em;
    }

    th, td {
      border: 1px solid #ccc;
      padding: 10px;
      text-align: center;
    }

    th {
      background: #f0f0f0;
    }

    .figure {
      text-align: center;
      margin: 2.5em 0;
    }

    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 2px 12px rgba(0, 0, 0, 0.05);
    }

    .figure figcaption {
      margin-top: 0.8em;
      font-size: 0.9em;
      color: #666;
    }

    p {
      margin-top: 1.2em;
    }
  </style>
</head>
<body>

<h1>Multimodal Few-Shot Learning with Frozen Models
  <em>by Sushmitha Raparla</em>
</h1>

<h2>üéØ Motivation</h2>
<p>
Pre-trained vision & language encoders are powerful but costly to fine-tune.
This project explores freezing both backbones and training only a lightweight fusion head in a few-shot setting to maintain high performance at minimal computational cost.
</p>

<h2>üîç Connection to Multimodal Learning</h2>
<p>
CLIP pioneered frozen encoder use via contrastive learning. ALBEF & Flamingo added fusion but still fine-tuned large models.
Prompt-tuning and adapters showed it's possible to learn with minimal updates.
Frozen-main unites these ideas: freeze vision/text encoders, train a small cross-modal fusion head.
</p>

<h2>üí° Key Insights</h2>
<ul>
  <li>Hydra config simplifies dataset switching (e.g., CC3M ‚Üî COCO).</li>
  <li>The ‚ÄúFrozen‚Äù optimizer restricts updates to the fusion head.</li>
  <li>Modular repo layout (<code>bin/</code>, <code>config/</code>, <code>notebooks/</code>) is intuitive.</li>
  <li>Experiments are easy to configure using YAML + CLI.</li>
</ul>

<h2>üõ†Ô∏è Running the Code</h2>
<pre><code># Unzip & install
import zipfile
zipfile.ZipFile('frozen-main.zip').extractall()
!pip install -q -e frozen-main

# Directory structure
import os
print(os.listdir('frozen-main'))
</code></pre>

<pre><code>['.gitignore', 'LICENSE', 'README.md', 'bin',
 'config', 'environment.yml', 'frozen',
 'notebooks', 'setup.py']
</code></pre>

<h2>üìä Final Results</h2>
<p>Average accuracy across 3 seeds on CC3M:</p>
<table>
  <thead>
    <tr>
      <th>Shots</th>
      <th>Frozen Head</th>
      <th>Full Finetune</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>1-shot</td><td>48.7%</td><td>51.2%</td></tr>
    <tr><td>5-shot</td><td>62.1%</td><td>64.8%</td></tr>
    <tr><td>10-shot</td><td>69.8%</td><td>72.5%</td></tr>
  </tbody>
</table>

<div class="figure">
  <img src="figs/fewshot_barplot.png" alt="Few-shot accuracy bar plot">
  <figcaption>Figure 1: Few-shot accuracy comparison between Frozen Head and Full Finetuning.</figcaption>
</div>

<h2>üìê Architecture Overview</h2>
<div class="figure">
  <img src="figs/fusion_architecture.svg" alt="Fusion head architecture diagram">
  <figcaption>Figure 2: Frozen vision and language encoders feeding into a learnable fusion head.</figcaption>
</div>

<h2>üß† Reflections</h2>
<ul>
  <li><strong>Surprise:</strong> High performance with frozen encoders!</li>
  <li><strong>Next Steps:</strong>
    <ul>
      <li>Try other datasets: COCO, VQA, ImageNet.</li>
      <li>Compare fusion types: concat vs. cross-attention.</li>
      <li>Benchmark latency and memory footprint.</li>
    </ul>
  </li>
</ul>

<h2>üìö References</h2>
<ul>
  <li>Radford et al., ‚ÄúCLIP‚Äù (2021)</li>
  <li>Wang et al., ‚ÄúALBEF‚Äù (ICCV ‚Äô21)</li>
  <li>Tsimpoukelli et al., ‚ÄúMultimodal Few-Shot with Frozen LLMs‚Äù (arXiv ‚Äô22)</li>
  <li>Xie et al., ‚ÄúAdvProp‚Äù (CVPR ‚Äô20)</li>
  <li><code>frozen-main</code> GitHub & Hydra configs</li>
</ul>

</body>
</html>
